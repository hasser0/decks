# Simple
what is a stream;; any data source that grows over time
spark stream sources;; landing files, updates to database, pub/sub messages
how to process streams;; reprocess everything or capture new data
options for trigger on writeStream;; processingTime, once, availableNow
how availableNow trigger works?;; process all data available in micro batch
options for outputMode on writeStream;; append, complete
why is checkpointLocation used in writeStream?;; to save state in case of failure
not supported operations in an stream;; sort, drop duplicates
what is incremental data ingestion?;; ability to load/refresh data from files automatically
when is copy into used?;; it is used to load new data files and skip old
copy into vs autoloader;; thousands vs millions files, less vs more efficient
why is necessary to use autoloader checkpoint?;; fault tolerance, avoid duplicated data, store metadata
multi hop architecture;;	data design pattern to organize data in multiple layers
basic layers in multi hop architecture;; bronze, silver, gold
what is bronze, silver and gold layers?;; ingested raw data, filtered/cleaned/augmented data and aggregated data
benefits of multi hop;; simple, incremental etl, streaming/batch processing and can recreate tables from raw_data

# Input
create a stream from a table|| spark.readStream.table("__table__")
copy into|| copy into __table__ from __path__ fileformat=__format__ format_options(__options__) copy_options(__option__)
autoloader read|| spark.readStream.format("cloudFiles").option("cloudFiles.format", __format__).option("cloudFiles.schemaLocation", __path__).load(__path__)
autoloader write|| streamDF.writeStream.option("checkpointLocation", __path__).option("mergeSchema", "true").table(__table__)