# Simple
what are spot instances;; instances that can be share for scalability with other workflows depending on demand
difference between autoloader and copy into;; copy into only support listing directory differences, while autoloader also support file notification (autoloader can automatically launch workflows)
when is more convenient to use copy into instead of autoloader?;; when you want to reprocess certain files
what is stored in schemaLocation?;; the inferred schema from previous loads
control plane is stored in...;; databricks cloud account
data plane is stored in...;; customer cloud account (azure, aws, gcp)
what does control plane store?;; high level data like notebooks, accounts, jobs, cluster management
what does data plane store or integrates?;; compute like ec2 or azure instances and data lake storage
what is a job cluster?;; a cluster that is created for a specific job and terminated when it's done
what cannot be done with repo api?;; auto pull branch on merge, PR, merge
how structured streaming support fault tolerance?;; with idempotent sinks and checkpoints
different ways to run a job;; cron, on demand
what does cluster size is and what it change?;; it changes the number of worker nodes per cluster from 2X-small(1 worker) up to 4X-large(128 workers) so that it can run faster
what does cluster scale out is and what it change?;; it changes the number of clusters, so you can run multiple cluster
we can have 2x-small clusters mixed with 4X-large clusters in the same warehouse;; False
insert overwrite deletes historical data;; False
option to propose column types to auto loader;; schemaHint
autoloader reprocess not loaded data;; True
jobs can be schedule by ...;; cron, on demand
delta live table pipelines can be schedule by as ...;; continuos, triggered
types of tasks;; notebook, dbt, python, delta live pipeline, spark submit
streaming write modes;; append, complete, update
what happens when you create a delta table constrain with violated records;; records are added to the target table and recorded as invalid in the event log

# Input
decorator to expect data quality|| @dlt.expect("__name__", "__expr__")
decorator to expect data quality or drop|| @dlt.expect_or_drop("__name__", "__expr__")
decorator to expect data quality or fail|| @dlt.expect_or_drop("__name__", "__expr__")
generate fields on ingestion ddl|| generate always as __expr__